{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from bayesian_torch.layers import LinearFlipout\n",
    "from bayesian_torch.models.dnn_to_bnn import get_kl_loss \n",
    "import psutil\n",
    "\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from utilities import plot_prediction_area_curves, get_pa_intersection"
   ],
   "id": "cf2f72ad7c0125ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "class BNNMineralProspectivity(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 = LinearFlipout(input_dim, 32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = LinearFlipout(32, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc_out = LinearFlipout(32, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        total_kl_divergence = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        current_x, layer1_kl = self.fc1(x) \n",
    "        total_kl_divergence += layer1_kl.to(x.device) \n",
    "        current_x = self.relu1(current_x)\n",
    "\n",
    "        current_x, layer2_kl = self.fc2(current_x)\n",
    "        total_kl_divergence += layer2_kl.to(x.device)\n",
    "        current_x = self.relu2(current_x)\n",
    "\n",
    "        logits, output_layer_kl = self.fc_out(current_x)\n",
    "        total_kl_divergence += output_layer_kl.to(x.device)\n",
    "            \n",
    "        return logits, total_kl_divergence"
   ],
   "id": "16cc491d33c18414"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "def test_outputs(\n",
    "    model: BNNMineralProspectivity,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_mc_samples: int = 50 \n",
    "):\n",
    "    model.eval() \n",
    "    all_true_labels_list, all_mean_probs_list, all_pred_variances_list = [], [], []\n",
    "    with torch.no_grad(): \n",
    "        for x_batch, y_batch in tqdm(test_loader, desc=\"Generating Test Outputs (MC Sampling)\"):\n",
    "            x_batch = x_batch.to(device) \n",
    "            \n",
    "            mc_logits_samples = []\n",
    "            for _ in range(num_mc_samples):\n",
    "                # Model's forward pass returns (logits, kld)\n",
    "                # Only need logits for prdictions.\n",
    "                logits_sample, _ = model(x_batch) \n",
    "                mc_logits_samples.append(logits_sample)\n",
    "            \n",
    "            mc_logits_stacked = torch.stack(mc_logits_samples)\n",
    "            mc_probs_stacked = torch.sigmoid(mc_logits_stacked)\n",
    "\n",
    "            mean_probs = mc_probs_stacked.mean(dim=0).squeeze()\n",
    "            pred_variance = mc_probs_stacked.var(dim=0).squeeze()\n",
    "            \n",
    "            if mean_probs.ndim == 0: mean_probs = mean_probs.unsqueeze(0)\n",
    "            if pred_variance.ndim == 0: pred_variance = pred_variance.unsqueeze(0)\n",
    "\n",
    "            all_true_labels_list.append(y_batch.cpu().numpy().flatten())\n",
    "            all_mean_probs_list.append(mean_probs.cpu().numpy().flatten()) \n",
    "            all_pred_variances_list.append(pred_variance.cpu().numpy().flatten())\n",
    "\n",
    "    all_true_labels = np.concatenate(all_true_labels_list)\n",
    "    all_mean_probs = np.concatenate(all_mean_probs_list)\n",
    "    all_pred_variances = np.concatenate(all_pred_variances_list)\n",
    "    all_pred_labels = (all_mean_probs > 0.5).astype(int)\n",
    "    \n",
    "    return all_true_labels, all_mean_probs, all_pred_labels, all_pred_variances\n"
   ],
   "id": "50f7603700d3a21f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "def mineral_classification_bnn_train(\n",
    "    train_features_path: str = './data/dataset_train.pt',\n",
    "    train_labels_path: str = './data/mineral_train.pt',\n",
    "    test_features_path: str = './data/dataset_test.pt',\n",
    "    test_labels_path: str = './data/mineral_test.pt',\n",
    "    output_dir_base_name: str = \"./bnn_mineral_outputs\", \n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 1e-3,\n",
    "    kl_weight_scale: float = 1.0, \n",
    "    random_state: int = 42, \n",
    "    print_every_epoch: int = 1, \n",
    "    num_mc_eval_epoch: int = 5\n",
    "):\n",
    "    RANDOM_SEED = random_state \n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    random.seed(RANDOM_SEED)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    output_dir = output_dir_base_name \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    X_train_np = torch.load(train_features_path).numpy()\n",
    "    y_train_np = torch.load(train_labels_path).numpy().ravel()\n",
    "    X_test_np = torch.load(test_features_path).numpy()\n",
    "    y_test_np = torch.load(test_labels_path).numpy().ravel() \n",
    "\n",
    "    X_train_np, X_test_np = X_train_np.astype(np.float32), X_test_np.astype(np.float32)\n",
    "    y_train_np, y_test_np = y_train_np.astype(np.float32), y_test_np.astype(np.float32)\n",
    "\n",
    "    adasyn = ADASYN(random_state=RANDOM_SEED) \n",
    "    X_train_np, y_train_np = adasyn.fit_resample(X_train_np, y_train_np) \n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train_np), torch.from_numpy(y_train_np).unsqueeze(1))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test_np), torch.from_numpy(y_test_np).unsqueeze(1))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Model, Optimizer, Criterion\n",
    "    input_dim = X_train_np.shape[1] \n",
    "    model = BNNMineralProspectivity(input_dim).to(device) \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) \n",
    "    criterion_nll = nn.BCEWithLogitsLoss() \n",
    "    kl_weight = kl_weight_scale / len(train_dataset) \n",
    "\n",
    "    #### TRAINING \n",
    "    train_losses_epoch, train_errors_epoch, test_errors_epoch, epochs_list = [], [], [], []\n",
    "    epoch_durations_list = []\n",
    "    \n",
    "    # For memory tracking\n",
    "    process = psutil.Process(os.getpid())\n",
    "    overall_peak_ram_mb = 0\n",
    "    \n",
    "    print(\"\\nStarting BNN training...\")\n",
    "    overall_training_start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time() \n",
    "        model.train()\n",
    "        \n",
    "        # Reset the peak tracker for each epoch\n",
    "        peak_ram_in_epoch_mb = 0\n",
    "        \n",
    "        epoch_total_loss, correct_train, total_train = 0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for x_batch, y_batch in pbar:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device) \n",
    "            optimizer.zero_grad() \n",
    "            logits, kl_div_from_model = model(x_batch)\n",
    "            nll_loss = criterion_nll(logits, y_batch) \n",
    "            elbo_loss = nll_loss + kl_weight * kl_div_from_model \n",
    "            elbo_loss.backward(); optimizer.step() \n",
    "\n",
    "            epoch_total_loss += elbo_loss.item()\n",
    "            probs = torch.sigmoid(logits); predicted_train = (probs > 0.5).float() \n",
    "            total_train += y_batch.size(0); correct_train += (predicted_train == y_batch).sum().item()\n",
    "            pbar.set_postfix({\"L\": elbo_loss.item()})\n",
    "            \n",
    "            # Update both epoch and overall peaks after each batch\n",
    "            current_ram_mb = process.memory_info().rss / (1024 ** 2)\n",
    "            peak_ram_in_epoch_mb = max(peak_ram_in_epoch_mb, current_ram_mb)\n",
    "            overall_peak_ram_mb = max(overall_peak_ram_mb, current_ram_mb)\n",
    "            \n",
    "        avg_epoch_loss = epoch_total_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train if total_train > 0 else 0\n",
    "        train_error = (1 - train_accuracy) * 100\n",
    "        train_losses_epoch.append(avg_epoch_loss); train_errors_epoch.append(train_error)\n",
    "        epochs_list.append(epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % print_every_epoch == 0 or epoch == num_epochs - 1:\n",
    "            print(f\"Epoch {epoch+1} - Avg Loss: {avg_epoch_loss:.4f}, Train Err: {train_error:.2f}%\")\n",
    "\n",
    "        current_epoch_duration = time.time() - epoch_start_time\n",
    "        epoch_durations_list.append(current_epoch_duration)\n",
    "        \n",
    "        # peak RAM for the epoch\n",
    "        print(f\"Epoch {epoch+1} completed in {current_epoch_duration:.2f} seconds. Peak RAM in Epoch: {peak_ram_in_epoch_mb:.2f} MB\")\n",
    "        \n",
    "    overall_training_duration = time.time() - overall_training_start_time\n",
    "    print(f\"\\nTotal training time: {overall_training_duration:.2f} seconds.\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"test_loader\": test_loader,\n",
    "        \"device\": device,\n",
    "        \"epochs_list\": epochs_list,\n",
    "        \"train_losses_epoch\": train_losses_epoch,\n",
    "        \"train_errors_epoch\": train_errors_epoch,\n",
    "        \"test_errors_epoch\": test_errors_epoch,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"RANDOM_SEED\": RANDOM_SEED,\n",
    "        \"num_epochs_trained\": num_epochs,\n",
    "        \"epoch_durations\": epoch_durations_list,\n",
    "        \"peak_ram_mb\": overall_peak_ram_mb \n",
    "    }"
   ],
   "id": "8c4d45c3dcebec0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_data_dir = './data' \n",
    "    run_output_dir_base = \"./bnn_mineral_outputs_run\" \n",
    "\n",
    "    training_results = mineral_classification_bnn_train(\n",
    "        train_features_path=os.path.join(base_data_dir,'dataset_train.pt'),\n",
    "        train_labels_path=os.path.join(base_data_dir,'mineral_train.pt'),\n",
    "        test_features_path=os.path.join(base_data_dir,'dataset_test.pt'),\n",
    "        test_labels_path=os.path.join(base_data_dir,'mineral_test.pt'),\n",
    "        output_dir_base_name=run_output_dir_base, \n",
    "        num_epochs=100, \n",
    "        batch_size=32,\n",
    "        learning_rate=1e-3,\n",
    "        kl_weight_scale=1,\n",
    "        random_state=42,\n",
    "        print_every_epoch=1,\n",
    "        num_mc_eval_epoch=1\n",
    "    )\n",
    "    \n",
    "    model_trained = training_results[\"model\"]\n",
    "    epoch_durations_list = training_results[\"epoch_durations\"]\n",
    "    peak_ram_mb = training_results[\"peak_ram_mb\"] \n",
    "\n",
    "    print(\"\\n--- Final Summary ---\")\n",
    "\n",
    "    # Average time per epoch\n",
    "    if epoch_durations_list:\n",
    "        avg_time = np.mean(epoch_durations_list)\n",
    "        std_time = np.std(epoch_durations_list)\n",
    "        print(f\"BNN - Average time per epoch: {avg_time:.2f} Â± {std_time:.2f} seconds\")\n",
    "\n",
    "    # Overall peak RAM\n",
    "    print(f\"BNN - Overall Peak RAM: {peak_ram_mb:.2f} MB\")\n",
    "\n",
    "    # Number of parameters\n",
    "    total_params = sum(p.numel() for p in model_trained.parameters() if p.requires_grad)\n",
    "    print(f\"BNN - Total trainable parameters: {total_params:,}\")"
   ],
   "id": "49dc25862f4b95fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "model_trained = training_results[\"model\"]\n",
    "test_loader_final = training_results[\"test_loader\"]\n",
    "device_final = training_results[\"device\"]\n",
    "epochs_list_final = training_results[\"epochs_list\"]\n",
    "train_losses_final = training_results[\"train_losses_epoch\"]\n",
    "train_errors_final = training_results[\"train_errors_epoch\"]\n",
    "test_errors_final = training_results[\"test_errors_epoch\"]\n",
    "output_dir_final = training_results[\"output_dir\"]\n",
    "RANDOM_SEED_final = training_results[\"RANDOM_SEED\"]\n",
    "num_epochs_completed = training_results[\"num_epochs_trained\"]\n",
    "epoch_durations_list = training_results[\"epoch_durations\"]"
   ],
   "id": "85f96e205d8b8a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 14,
   "source": "num_mc_for_detailed_eval = 100",
   "id": "33a39aed83c3d4a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(epochs_list_final, train_errors_final, '-', label='Training Error')\n",
    "\n",
    "mask = ~np.isnan(test_errors_final)\n",
    "if mask.any():\n",
    "    plt.plot(\n",
    "        np.array(epochs_list_final)[mask],\n",
    "        np.array(test_errors_final)[mask],\n",
    "        '-', label='Test Error')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error Rate (%)')\n",
    "plt.title('BNN Error Rate vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "outfile = os.path.join(output_dir_final, 'bnn_error_rate_vs_epoch.png')\n",
    "plt.tight_layout()\n",
    "plt.savefig(outfile)\n",
    "plt.show()"
   ],
   "id": "392eff2fa05cdc21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_save_path_final = os.path.join(output_dir_final, 'bnn_mpm_adasyn.pth')\n",
    "torch.save(model_trained.state_dict(), model_save_path_final)"
   ],
   "id": "95e6f1148267b8da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "true_labels, mean_probs, pred_labels, pred_variances = test_outputs(\n",
    "    model_trained, test_loader_final, device_final, num_mc_samples=num_mc_for_detailed_eval)\n",
    "\n",
    "np.save(os.path.join(output_dir_final, 'bnn_true_labels.npy'), true_labels)\n",
    "np.save(os.path.join(output_dir_final, 'bnn_mean_probs.npy'), mean_probs)\n",
    "np.save(os.path.join(output_dir_final, 'bnn_pred_labels.npy'), pred_labels)\n",
    "np.save(os.path.join(output_dir_final, 'bnn_pred_variances.npy'), pred_variances)"
   ],
   "id": "3ce16ef301bf0fd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "final_test_error_rate = (1.0 - np.sum(pred_labels == true_labels) / len(true_labels)) * 100 if len(true_labels) > 0 else 0.0\n",
    "fpr, tpr, _ = roc_curve(true_labels, mean_probs) \n",
    "roc_auc_score = auc(fpr, tpr) if len(fpr) > 1 and len(tpr) > 1 else 0.0"
   ],
   "id": "79f92a7ae5a84e7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_dir = \"./bnn_mineral_outputs\" \n",
    "\n",
    "true_labels = np.load(os.path.join(output_dir, 'bnn_true_labels.npy'))\n",
    "mean_probs = np.load(os.path.join(output_dir, 'bnn_mean_probs.npy'))\n",
    "pred_labels = np.load(os.path.join(output_dir, 'bnn_pred_labels.npy'))\n",
    "pred_variances = np.load(os.path.join(output_dir, 'bnn_pred_variances.npy')) \n",
    "\n",
    "print(f\"True labels shape: {true_labels.shape}\")\n",
    "print(f\"Mean probabilities shape: {mean_probs.shape}\")\n",
    "print(f\"Predicted labels shape: {pred_labels.shape}\")\n",
    "print(f\"Predicted variances shape: {pred_variances.shape}\")"
   ],
   "id": "2e2e94cde237071b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fpr, tpr, roc_thresholds = roc_curve(true_labels, mean_probs)\n",
    "roc_auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'BNN ROC (AUC = {roc_auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "font_size = 14\n",
    "\n",
    "plt.xlabel('False positive rate', fontsize=font_size, labelpad=10)\n",
    "plt.ylabel('True positive rate', fontsize=font_size, labelpad=10)\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "plt.legend(loc=\"lower right\", fontsize=font_size)\n",
    "\n",
    "#plt.grid(True)\n",
    "plt.tight_layout()  \n",
    "plt.savefig(os.path.join(output_dir, 'bnn_roc_curve_final.pdf'), dpi=300, bbox_inches='tight') \n",
    "plt.show()"
   ],
   "id": "2f3cd2bb7cb22065"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Barren', 'Mineral'])\n",
    "fig_cm, ax_cm = plt.subplots(figsize=(8, 6))\n",
    "display.plot(ax=ax_cm, cmap=plt.cm.Blues)\n",
    "\n",
    "font_size = 14\n",
    "\n",
    "ax_cm.set_xlabel('Predicted label', fontsize=font_size, labelpad=10)\n",
    "ax_cm.set_ylabel('True label', fontsize=font_size, labelpad=10)\n",
    "\n",
    "ax_cm.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "cbar = ax_cm.images[0].colorbar\n",
    "if cbar:\n",
    "    cbar.ax.tick_params(labelsize=font_size)\n",
    "\n",
    "for text in ax_cm.texts:\n",
    "    text.set_fontsize(font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'bnn_confusion_matrix_final.pdf'), dpi=300, bbox_inches='tight') \n",
    "plt.show()"
   ],
   "id": "301e1a45b1eead45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(classification_report(true_labels, pred_labels, target_names=['Class 0', 'Class 1'], zero_division=0))",
   "id": "de353090745a5bf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "96a5e25a85dd3cbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation on noisy data",
   "id": "a523fdd7b274ec4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_test_scaled = torch.load(os.path.join(DATA_DIR, 'dataset_test.pt')).numpy()\n",
    "y_test = torch.load(os.path.join(DATA_DIR, 'mineral_test.pt')).numpy().ravel()\n",
    "\n",
    "np.random.seed(RANDOM_STATE) \n",
    "noise = np.random.normal(0, 0.1, X_test_scaled.shape).astype(np.float32)\n",
    "X_test_noisy_scaled = X_test_scaled + noise\n",
    "\n",
    "noisy_dataset = TensorDataset(torch.from_numpy(X_test_noisy_scaled), torch.from_numpy(y_test).unsqueeze(1))\n",
    "noisy_loader = DataLoader(noisy_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "noisy_labels, noisy_probs, _, _ = test_outputs(model, noisy_loader, device, num_mc_samples=NUM_MC_SAMPLES_EVAL)\n",
    "\n",
    "fpr_noisy, tpr_noisy, _ = roc_curve(noisy_labels, noisy_probs)\n",
    "roc_auc_noisy = auc(fpr_noisy, tpr_noisy)\n",
    "\n",
    "print(f\"Baseline ROC AUC on clean data: {roc_auc:.4f}\")\n",
    "print(f\"ROC AUC on noisy data:        {roc_auc_noisy:.4f}\")\n",
    "print(f\"Performance Drop (AUC Drop):  {roc_auc - roc_auc_noisy:.4f}\")"
   ],
   "id": "ab7c9c3f54197cd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f848d4fc8a2e607e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
